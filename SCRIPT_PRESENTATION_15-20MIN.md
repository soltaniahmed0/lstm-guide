# Script de Pr√©sentation - LSTM (Version 15-20 minutes)

**Pr√©sentateurs :** RAJA HANNACHI & AHMED SOLTANI  
**Encadr√© par :** M. MOHAMED RIDHA AMAMOU  
**Module :** Machine Learning  
**Ann√©e Universitaire :** 2025-2026

---

## SLIDE 1 : TITRE (30 secondes)

Bonjour √† tous,

Nous vous pr√©sentons aujourd'hui notre travail sur les LSTM - Long Short-Term Memory, dans le cadre du module Machine Learning.

---

## SLIDE 2 : PLAN (30 secondes)

Notre pr√©sentation couvrira :
1. Introduction avec un exemple concret
2. RNN : d√©finition, architecture et probl√®mes
3. LSTM : solution et architecture
4. √âtude de cas : pr√©diction du prix de l'or
5. Conclusion et perspectives

---

## SLIDE 3 : INTRODUCTION (1 minute 30)

**Le D√©fi** : Pr√©dire les prix de l'or pour optimiser les investissements.

**Le Probl√®me** : Les prix d√©pendent de facteurs complexes et interd√©pendants sur de longues p√©riodes.

**Solution Traditionnelle** : Les mod√®les classiques sont limit√©s.

**Solution LSTM** : Les LSTM apprennent ces patterns complexes avec **96% de pr√©cision**.

**R√©sultat** : D√©cisions √©clair√©es, risques r√©duits, profits maximis√©s.

---

## SLIDE 4 : RNN - D√âFINITION (1 minute)

Un RNN est un r√©seau de neurones con√ßu pour traiter des **s√©quences de donn√©es** o√π l'ordre temporel est important.

**Point cl√©** : Contrairement aux r√©seaux classiques, les RNN utilisent leur sortie pr√©c√©dente comme entr√©e, cr√©ant une **m√©moire interne**.

**Analogie** : Comme lire un livre - on se souvient du contexte pr√©c√©dent pour comprendre le pr√©sent.

**Applications** : Traitement du langage naturel, s√©ries temporelles, reconnaissance vocale.

---

## SLIDE 5-7 : RNN - ARCHITECTURE (2 minutes)

**Formule principale** :
```
h‚Çú = tanh(W‚Çï‚Çï ¬∑ h‚Çú‚Çã‚ÇÅ + W‚Çì‚Çï ¬∑ x‚Çú + b‚Çï)
y‚Çú = W‚Çï·µß ¬∑ h‚Çú + b·µß
```

L'√©tat cach√© h‚Çú combine l'√©tat pr√©c√©dent et l'input actuel. C'est la **m√©moire** du r√©seau.

**Flux** : Entr√©es ‚Üí Multiplications par poids ‚Üí Addition ‚Üí tanh ‚Üí Sortie ‚Üí Boucle r√©currente

Dans la pratique, x est un **vecteur** et les op√©rations sont des **multiplications matricielles**.

---

## SLIDE 8-9 : RNN - PROBL√àMES (2 minutes 30)

**PROBL√àME 1 : VANISHING GRADIENT**

Lors de la r√©tropropagation, les gradients deviennent tr√®s petits (presque nuls) en remontant dans le temps.

**Pourquoi ?** W‚Çï‚Çï < 1 et tanh' ‚â§ 1 ‚Üí chaque multiplication r√©duit le gradient.

**Cons√©quence** : Apr√®s 4 timesteps, le gradient n'est plus que **0.39%** de sa valeur initiale ! Le r√©seau n'apprend plus.

**PROBL√àME 2 : EXPLODING GRADIENT**

Quand W‚Çï‚Çï > 1, les gradients explosent ‚Üí instabilit√© num√©rique, mod√®le diverge.

**Solution** : C'est pour cela que les **LSTM** ont √©t√© cr√©√©s !

---

## SLIDE 10 : LSTM - D√âFINITION (1 minute)

**LSTM** = Long Short-Term Memory

Un type sp√©cial de RNN qui r√©sout le probl√®me du Vanishing Gradient.

**Comparaison** :
- **RNN** : Oublie rapidement, limite √† ~10 pas de temps
- **LSTM** : M√©morise sur de longues s√©quences, peut traiter des centaines de pas

---

## SLIDE 11 : LSTM - GATES (1 minute 30)

L'innovation cl√© : **5 composants** qui contr√¥lent le flux d'information :

1. **Forget Gate** : D√©cide quoi oublier
2. **Input Gate** : D√©cide quoi apprendre
3. **Candidate Gate** : G√©n√®re nouvelles valeurs
4. **Cell State** : M√©moire √† long terme
5. **Output Gate** : D√©cide quoi produire

Ces portes permettent au gradient de circuler sans dispara√Ætre.

---

## SLIDE 12 : LSTM - ARCHITECTURE (1 minute 30)

**Flux LSTM** :
- Entr√©es : C‚Çú‚Çã‚ÇÅ, h‚Çú‚Çã‚ÇÅ, x‚Çú
- 3 portes (sigmoid) + 1 candidate (tanh)
- Cell State traverse le temps sans d√©gradation
- Sorties : C‚Çú, h‚Çú

**Avantages** :
- Oublie s√©lectivement (Forget Gate)
- Apprend s√©lectivement (Input Gate)
- M√©morise long terme (Cell State)
- Produit sorties pertinentes (Output Gate)

---

## SLIDE 13 : √âTUDE DE CAS (3 minutes)

**Application** : Pr√©diction du prix de l'or (Kaggle)

**Architecture** :
- 3 couches LSTM de 50 neurones
- Dropout 0.2 pour √©viter surapprentissage
- S√©quences de 60 jours pour pr√©dire le jour suivant

**R√©sultats** :
- **Pr√©cision : 96%**
- **MAE : ~4%**
- **R¬≤ Score : > 0.95**

**√âtapes cl√©s** :
1. Normalisation Min-Max
2. Cr√©ation s√©quences (60 jours)
3. Entra√Ænement avec Adam optimizer
4. √âvaluation avec m√©triques standard

D√©monstration pratique de l'efficacit√© des LSTM !

---

## SLIDE 14 : CONCLUSION - R√âSUM√â (1 minute)

**RNN** : R√©seaux r√©currents, probl√®me Vanishing Gradient, limite ~10 pas.

**LSTM** : Solution avec 5 neurones (gates), traite des centaines de pas.

**√âtude de Cas** : 96% de pr√©cision sur pr√©diction prix de l'or.

---

## SLIDE 15 : CONCLUSION - COMPARAISON (1 minute 30)

**LSTM vs Transformers** :

**LSTM** : S√©quentiel, quelques centaines de tokens, mod√©r√©, bon pour s√©ries temporelles.

**Transformers** : Parall√®le, plusieurs milliers de tokens, complexe, dominant en NLP.

**Quand utiliser LSTM ?**
- S√©ries temporelles univari√©es
- Ressources limit√©es
- Applications temps r√©el

**Quand utiliser Transformers ?**
- NLP avanc√©
- Tr√®s longues s√©quences
- Ressources importantes

---

## SLIDE 16 : CONCLUSION - PERSPECTIVES (1 minute)

**√âvolutions** : Hybridation LSTM+Transformers, mod√®les plus efficaces.

**Applications** : M√©decine pr√©dictive, finance algorithmique, IoT.

**D√©fis** : Interpr√©tabilit√©, consommation √©nerg√©tique, √©thique.

---

## SLIDE 17 : WEBographie (15 secondes)

Voici nos r√©f√©rences principales.

---

## SLIDE 18 : MERCI (15 secondes)

Merci pour votre attention ! Questions ?

---

## TIMING D√âTAILL√â (Total : 18 minutes)

| Slide | Contenu | Temps |
|-------|---------|-------|
| 1 | Titre | 0:30 |
| 2 | Plan | 0:30 |
| 3 | Introduction | 1:30 |
| 4 | RNN D√©finition | 1:00 |
| 5-7 | RNN Architecture | 2:00 |
| 8-9 | RNN Probl√®mes | 2:30 |
| 10 | LSTM D√©finition | 1:00 |
| 11 | LSTM Gates | 1:30 |
| 12 | LSTM Architecture | 1:30 |
| 13 | √âtude de Cas | 3:00 |
| 14 | Conclusion R√©sum√© | 1:00 |
| 15 | Comparaison | 1:30 |
| 16 | Perspectives | 1:00 |
| 17 | Webographie | 0:15 |
| 18 | Merci | 0:15 |
| | **TOTAL** | **18:00** |

**Buffer pour questions : 2 minutes** ‚Üí **Total max : 20 minutes**

---

## CONSEILS POUR LA PR√âSENTATION

### ‚è±Ô∏è Gestion du temps
- **Parlez √† un rythme mod√©r√©** : ~150 mots/minute
- **Utilisez les animations** : Elles expliquent mieux que les mots
- **Passez rapidement** sur les d√©tails techniques si vous √™tes en retard
- **Gardez 2 minutes** pour les questions √† la fin

### üéØ Points essentiels √† ne PAS manquer
1. **Le probl√®me** : Vanishing Gradient (slide 8-9)
2. **La solution** : Les gates LSTM (slide 11)
3. **L'application** : 96% de pr√©cision (slide 13)

### üí° Astuces
- **Montrez les animations** : Laissez-les jouer, elles sont explicites
- **Soyez concis** : Ne r√©p√©tez pas ce qui est d√©j√† sur les slides
- **Interagissez** : Posez des questions rh√©toriques ("Pourquoi ?", "Comment ?")
- **Concluez fort** : Terminez sur les r√©sultats de l'√©tude de cas

### ‚ö†Ô∏è Si vous √™tes en retard
**√Ä raccourcir en priorit√©** :
- Slide 5-7 (Architecture RNN) : Passez rapidement
- Slide 12 (Architecture LSTM) : Montrez juste le sch√©ma
- Slide 15 (Comparaison) : Mentionnez juste les points cl√©s

**√Ä garder absolument** :
- Slide 3 (Introduction) : Hook important
- Slide 8-9 (Probl√®mes) : Le c≈ìur du probl√®me
- Slide 11 (Gates) : La solution cl√©
- Slide 13 (√âtude de cas) : Preuve concr√®te

---

## VERSION ULTRA-RAPIDE (15 minutes)

Si vous devez absolument tenir 15 minutes :

| Section | Temps |
|---------|-------|
| Titre + Plan | 0:45 |
| Introduction | 1:00 |
| RNN (tout) | 4:00 |
| LSTM (tout) | 4:00 |
| √âtude de Cas | 3:00 |
| Conclusion | 1:30 |
| Merci | 0:45 |
| **TOTAL** | **15:00** |

**Strat√©gie** :
- Parlez plus vite sur les slides techniques
- Passez rapidement sur les d√©tails math√©matiques
- Concentrez-vous sur les concepts cl√©s
- Utilisez les animations comme support visuel principal

---

**Bonne pr√©sentation ! üéØ**

**Rappel** : Le timing est indicatif. Adaptez selon votre rythme et les r√©actions de l'audience.
