# Script de Pr√©sentation - Long Short-Term Memory (LSTM)

**Pr√©sentateurs :** RAJA HANNACHI & AHMED SOLTANI  
**Encadr√© par :** M. MOHAMED RIDHA AMAMOU  
**Module :** Machine Learning  
**Ann√©e Universitaire :** 2025-2026

---

## SLIDE 1 : TITRE

Bonjour √† tous,

Nous sommes ravis de vous pr√©senter aujourd'hui notre travail sur les r√©seaux de neurones r√©currents, et plus particuli√®rement sur les LSTM - Long Short-Term Memory.

Cette pr√©sentation a √©t√© r√©alis√©e dans le cadre du module Machine Learning, sous l'encadrement de M. Mohamed Ridha Amamou.

---

## SLIDE 2 : PLAN DE PR√âSENTATION

Avant de commencer, voici le plan de notre pr√©sentation :

1. **Introduction** : Nous commencerons par un exemple concret qui illustre l'utilit√© des LSTM
2. **RNN** : Nous explorerons les r√©seaux de neurones r√©currents, leur d√©finition, leur architecture, leur fonctionnement, leurs applications et leurs limitations
3. **LSTM** : Nous verrons comment les LSTM r√©solvent les probl√®mes des RNN, leur d√©finition, leur architecture et leurs portes
4. **√âtude de Cas** : Nous pr√©senterons une application pratique : la pr√©diction du prix de l'or avec une pr√©cision de 96%
5. **Conclusion** : Nous ferons un r√©sum√© et discuterons des perspectives, notamment la comparaison avec les Transformers
6. **Webographie** : Nous terminerons par nos r√©f√©rences

---

## SLIDE 3 : INTRODUCTION

Commen√ßons par un exemple concret qui illustre parfaitement le d√©fi que nous essayons de r√©soudre.

**Le D√©fi** : Imaginez que vous √™tes un investisseur et que vous souhaitez pr√©dire les prix de l'or pour optimiser vos investissements. C'est un probl√®me r√©el et complexe.

**Le Probl√®me** : Les prix de l'or d√©pendent de nombreux facteurs interd√©pendants : les tendances historiques, les √©v√©nements √©conomiques mondiaux, les cycles saisonniers, et bien d'autres. Ces d√©pendances sont complexes et difficiles √† mod√©liser.

**Solution Traditionnelle** : Les mod√®les classiques comme la r√©gression lin√©aire ou les mod√®les ARIMA sont limit√©s. Ils ne peuvent pas capturer ces d√©pendances complexes sur de longues p√©riodes.

**Solution LSTM** : C'est l√† que les LSTM entrent en jeu. Ils peuvent apprendre ces patterns complexes et atteindre une pr√©cision de 96% dans la pr√©diction des prix de l'or.

**R√©sultat** : Avec cette pr√©cision, on peut prendre des d√©cisions √©clair√©es, r√©duire les risques et maximiser les profits.

Cet exemple illustre parfaitement la puissance des LSTM pour traiter des donn√©es s√©quentielles complexes.

---

## SLIDE 4 : RNN - D√âFINITION

Avant de parler des LSTM, il est essentiel de comprendre les RNN - R√©seaux de Neurones R√©currents.

**Qu'est-ce qu'un RNN ?**

Un RNN est un type de r√©seau de neurones artificiels con√ßu pour traiter des s√©quences de donn√©es o√π l'ordre et le contexte temporel sont importants.

**Le point cl√©** : Contrairement aux r√©seaux de neurones classiques, les RNN peuvent utiliser leur sortie pr√©c√©dente comme entr√©e, cr√©ant ainsi une m√©moire interne.

**Analogie** : C'est comme lire un livre. Vous vous souvenez de ce que vous avez lu pr√©c√©demment pour comprendre la phrase actuelle. Un RNN fait la m√™me chose avec les donn√©es s√©quentielles - il utilise le contexte pass√© pour traiter l'information pr√©sente.

---

## SLIDE 5 : RNN - CAS D'USAGE

Les RNN sont particuli√®rement adapt√©s √† plusieurs types de probl√®mes :

- **Traitement du langage naturel** : Analyse de sentiment, traduction automatique
- **S√©ries temporelles** : Pr√©diction de prix, pr√©visions m√©t√©orologiques
- **Reconnaissance vocale** : Transcription de la parole en texte
- **G√©n√©ration de texte** : Cr√©ation de contenu automatique

---

## SLIDE 6 : RNN - APPLICATIONS

Les RNN peuvent √™tre configur√©s de diff√©rentes mani√®res selon l'application :

- **Many-to-Many** : Traduction automatique (s√©quence d'entr√©e ‚Üí s√©quence de sortie)
- **Many-to-One** : Analyse de sentiment (s√©quence d'entr√©e ‚Üí une seule sortie)
- **One-to-Many** : G√©n√©ration de texte (une entr√©e ‚Üí s√©quence de sortie)

---

## SLIDE 7 : RNN - ARCHITECTURE (FORMULES)

Maintenant, regardons l'architecture math√©matique d'un RNN.

**Hidden State (√âtat Cach√©) - h‚Çú** :
```
h‚Çú = tanh(W‚Çï‚Çï ¬∑ h‚Çú‚Çã‚ÇÅ + W‚Çì‚Çï ¬∑ x‚Çú + b‚Çï)
```

L'√©tat cach√© combine l'√©tat pr√©c√©dent multipli√© par W‚Çï‚Çï avec l'input actuel multipli√© par W‚Çì‚Çï, plus un biais. La fonction tanh normalise le r√©sultat entre -1 et 1. C'est la m√©moire du r√©seau.

**Output (Sortie) - y‚Çú** :
```
y‚Çú = W‚Çï·µß ¬∑ h‚Çú + b·µß
```

La sortie est simplement une transformation lin√©aire de l'√©tat cach√©.

---

## SLIDE 8 : RNN - ARCHITECTURE (SCH√âMA ANIM√â)

Voici une visualisation anim√©e de l'architecture RNN. Vous pouvez voir comment les donn√©es circulent :

1. Les entr√©es h‚Çú‚Çã‚ÇÅ et x‚Çú sont re√ßues
2. Elles sont multipli√©es par leurs poids respectifs W‚Çï‚Çï et W‚Çì‚Çï
3. Les r√©sultats sont additionn√©s
4. La fonction tanh est appliqu√©e pour obtenir h‚Çú
5. h‚Çú est transform√© pour produire y‚Çú
6. h‚Çú est √©galement renvoy√© comme h‚Çú‚Çã‚ÇÅ pour le prochain timestep

Cette boucle r√©currente permet au r√©seau de maintenir une m√©moire des √©tats pr√©c√©dents.

---

## SLIDE 9 : RNN - ARCHITECTURE (VECTEURS)

Dans la pratique, x est g√©n√©ralement un vecteur, pas un simple nombre. Par exemple :
- Pour le traitement de texte : x peut √™tre un vecteur de dimension 100 (word embedding)
- Pour les s√©ries temporelles : x peut √™tre un vecteur de dimension 1

Les op√©rations sont donc des multiplications matricielles :
- W‚Çì‚Çï est une matrice de forme (R, M) o√π R est le nombre de neurones cach√©s et M la dimension de l'input
- W‚Çï‚Çï est une matrice carr√©e de forme (R, R)

---

## SLIDE 10 : RNN - FONCTIONNEMENT (CALCULS)

Regardons un exemple concret avec des valeurs r√©elles. Supposons :
- W‚Çï‚Çï = 0.5
- W‚Çì‚Çï = 0.8
- b‚Çï = 0.1
- h‚Çú‚Çã‚ÇÅ = 0.0
- x‚Çú = 1.0

Le calcul donne :
- h‚Çú = tanh(0.5 √ó 0.0 + 0.8 √ó 1.0 + 0.1) = tanh(0.9) ‚âà 0.716
- y‚Çú = 1.2 √ó 0.716 + 0.0 ‚âà 0.859

Ces calculs se r√©p√®tent √† chaque timestep, permettant au r√©seau d'apprendre des patterns temporels.

---

## SLIDE 11 : RNN - PROBL√àME : VANISHING GRADIENT

Malheureusement, les RNN souffrent d'un probl√®me majeur : le **Vanishing Gradient** - la disparition du gradient.

**Qu'est-ce que c'est ?**

Lors de la r√©tropropagation, les gradients deviennent tr√®s petits, presque nuls, au fur et √† mesure qu'on remonte dans le temps.

**Pourquoi ?**

Parce que W‚Çï‚Çï < 1 et tanh'(z) ‚â§ 1, donc chaque multiplication r√©duit le gradient. Par exemple, si W‚Çï‚Çï = 0.5 et tanh' ‚âà 0.5, chaque terme vaut environ 0.25.

**Cons√©quences** :
- Apr√®s 4 timesteps, le gradient n'est plus que 0.39% de sa valeur initiale !
- Apr√®s 10 timesteps, il est pratiquement nul
- Le r√©seau n'apprend plus car les poids ne se mettent presque plus √† jour
- Pas de m√©moire √† long terme : le r√©seau ne peut pas apprendre des d√©pendances distantes

**Solutions** : C'est exactement pour cela que les LSTM ont √©t√© cr√©√©s !

---

## SLIDE 12 : RNN - PROBL√àME : EXPLODING GRADIENT

Il existe aussi le probl√®me inverse : l'**Exploding Gradient** - l'explosion du gradient.

**Qu'est-ce que c'est ?**

Lorsque W‚Çï‚Çï > 1, les gradients deviennent tr√®s grands pendant la r√©tropropagation.

**Exemple** : Si W‚Çï‚Çï = 2.0, apr√®s 4 timesteps, le gradient est 16 fois plus grand qu'au d√©part !

**Cons√©quences** :
- Instabilit√© num√©rique : les valeurs deviennent NaN ou Infinity
- Mises √† jour trop grandes : les poids changent de mani√®re erratique
- Perte qui explose : la fonction de perte augmente exponentiellement
- Impossibilit√© d'entra√Æner : le mod√®le ne peut pas converger

**Solutions** :
- Gradient Clipping : limiter la valeur maximale du gradient
- LSTM/GRU : utiliser des architectures qui contr√¥lent mieux le flux du gradient
- Initialisation appropri√©e des poids

---

## SLIDE 13 : LSTM - D√âFINITION

Maintenant, passons aux LSTM - la solution aux probl√®mes des RNN.

**Qu'est-ce que LSTM ?**

LSTM (Long Short-Term Memory) est un type sp√©cial de RNN con√ßu pour r√©soudre le probl√®me du Vanishing Gradient et permettre au r√©seau de se souvenir d'informations sur de tr√®s longues s√©quences.

**Pourquoi LSTM ?**

**RNN Classique** :
- Oublie rapidement (Vanishing Gradient)
- Limite √† ~10 pas de temps
- Ne peut pas apprendre des d√©pendances longues

**LSTM** :
- M√©morise sur de longues s√©quences
- Peut traiter des centaines de pas
- Apprend des d√©pendances complexes

---

## SLIDE 14 : LSTM - GATES (5 NEURONES)

L'innovation cl√© des LSTM r√©side dans leur architecture avec 5 composants principaux :

1. **Forget Gate (Porte d'Oubli)** : D√©cide quelle information oublier de l'√©tat pr√©c√©dent
2. **Input Gate (Porte d'Entr√©e)** : D√©cide quelle nouvelle information stocker
3. **Candidate Gate (Porte Candidate)** : G√©n√®re les nouvelles valeurs candidates
4. **Cell State (√âtat de Cellule)** : La m√©moire √† long terme qui traverse le temps
5. **Output Gate (Porte de Sortie)** : D√©cide quelle partie de l'√©tat de cellule utiliser pour la sortie

Ces 5 neurones travaillent ensemble pour contr√¥ler pr√©cis√©ment le flux d'information, permettant au gradient de circuler sans dispara√Ætre.

---

## SLIDE 15 : LSTM - ARCHITECTURE

Voici une visualisation interactive de l'architecture LSTM. Vous pouvez voir :

- Les entr√©es : C‚Çú‚Çã‚ÇÅ (Cell State pr√©c√©dent), h‚Çú‚Çã‚ÇÅ (Hidden State pr√©c√©dent), et x‚Çú (input actuel)
- Les 3 portes (Forget, Input, Output) qui utilisent la fonction sigmoid
- La porte candidate qui utilise tanh
- Le flux de donn√©es √† travers les multiplications et additions
- Les sorties : C‚Çú (nouveau Cell State) et h‚Çú (nouveau Hidden State)

Cette architecture complexe permet au LSTM de :
- **Oublier** s√©lectivement des informations anciennes (Forget Gate)
- **Apprendre** de nouvelles informations (Input Gate)
- **M√©moriser** sur de longues p√©riodes (Cell State)
- **Produire** des sorties pertinentes (Output Gate)

---

## SLIDE 16 : √âTUDE DE CAS - PR√âDICTION DU PRIX DE L'OR

Passons maintenant √† une application pratique : la pr√©diction du prix de l'or avec LSTM.

**Source** : Cette √©tude de cas est bas√©e sur un projet Kaggle qui a atteint 96% de pr√©cision.

**√âtapes de l'impl√©mentation** :

1. **Importation des biblioth√®ques** : NumPy, Pandas, TensorFlow/Keras
2. **Chargement et pr√©paration** : Normalisation Min-Max des donn√©es
3. **Cr√©ation des s√©quences** : 60 jours de donn√©es pour pr√©dire le jour suivant
4. **Architecture du mod√®le** : 3 couches LSTM de 50 neurones chacune, avec Dropout
5. **Compilation et entra√Ænement** : Optimiseur Adam, fonction de perte MSE
6. **Pr√©diction** : Utilisation du mod√®le pour pr√©dire les prix futurs
7. **√âvaluation** : Calcul de la pr√©cision, MAE, et R¬≤ score

**R√©sultats** :
- **Pr√©cision** : 96%
- **MAE** : ~4%
- **R¬≤ Score** : > 0.95

Ces r√©sultats d√©montrent l'efficacit√© des LSTM pour les s√©ries temporelles financi√®res.

---

## SLIDE 17 : CONCLUSION - R√âSUM√â

Faisons un r√©sum√© de ce que nous avons couvert :

**RNN** :
- R√©seaux r√©currents pour s√©quences
- Probl√®me : Vanishing/Exploding Gradient
- Limite : ~10 pas de temps
- Applications : NLP, s√©ries temporelles

**LSTM** :
- Solution au probl√®me des RNN
- 5 neurones : 3 gates + Cell State + Hidden State
- Peut traiter des centaines de pas
- Applications : Traduction, pr√©diction, NLP

**√âtude de Cas** :
- Pr√©diction du prix de l'or
- Architecture : 3 couches LSTM
- R√©sultat : 96% de pr√©cision
- D√©monstration pratique r√©ussie

---

## SLIDE 18 : CONCLUSION - COMPARAISON LSTM VS TRANSFORMERS

Maintenant, comparons les LSTM avec les Transformers, l'architecture moderne qui domine actuellement le NLP.

**LSTM** :
- Architecture r√©currente (s√©quentielle)
- Traitement s√©quentiel (pas par pas)
- M√©moire : Cell State + Hidden State
- Longueur : Quelques centaines
- Vitesse : Lente (s√©quentielle)
- Complexit√© : Mod√©r√©e

**Transformers** :
- Architecture attention (parall√®le)
- Traitement parall√®le (tous les tokens)
- M√©moire : Attention Mechanism
- Longueur : Plusieurs milliers
- Vitesse : Rapide (parall√®le)
- Complexit√© : √âlev√©e

**Quand utiliser quoi ?**

**Utilisez LSTM pour** :
- S√©ries temporelles univari√©es
- Donn√©es s√©quentielles courtes √† moyennes
- Ressources limit√©es
- Mod√®les plus simples √† comprendre
- Applications temps r√©el

**Utilisez Transformers pour** :
- NLP avanc√© (traduction, g√©n√©ration)
- Tr√®s longues s√©quences
- Ressources computationnelles importantes
- Mod√®les de pointe (GPT, BERT)
- Attention explicite n√©cessaire

---

## SLIDE 19 : CONCLUSION - PERSPECTIVES

Regardons les perspectives futures :

**√âvolutions** :
- **Hybridation** : Combinaison LSTM + Transformers
- **Efficacit√©** : Mod√®les plus l√©gers et rapides
- **Domaines** : Expansion vers nouveaux domaines
- **Hardware** : Optimisation pour GPU/TPU

**Applications √âmergentes** :
- M√©decine pr√©dictive
- Finance algorithmique
- IoT et capteurs
- Reconnaissance vocale avanc√©e

**D√©fis** :
- Interpr√©tabilit√© des mod√®les
- Consommation √©nerg√©tique
- Biais et √©thique
- G√©n√©ralisation

---

## SLIDE 20 : WEBographie

Voici nos principales r√©f√©rences et ressources utilis√©es pour cette pr√©sentation.

[Les r√©f√©rences seront list√©es sur cette slide]

---

## SLIDE 21 : MERCI

Merci beaucoup pour votre attention !

Nous sommes maintenant disponibles pour r√©pondre √† vos questions.

---

## NOTES POUR LA PR√âSENTATION

### Timing sugg√©r√© :
- **Titre** : 30 secondes
- **Plan** : 1 minute
- **Introduction** : 2 minutes
- **RNN (toutes les slides)** : 10-12 minutes
- **LSTM (toutes les slides)** : 8-10 minutes
- **√âtude de Cas** : 5-7 minutes
- **Conclusion** : 3-5 minutes
- **Questions** : 5-10 minutes

**Total : ~35-45 minutes**

### Conseils de pr√©sentation :
1. **Parlez lentement et clairement** : Les concepts sont complexes, prenez votre temps
2. **Utilisez les animations** : Les slides interactives sont l√† pour aider, utilisez-les
3. **Faites des pauses** : Apr√®s chaque section importante, faites une pause pour laisser le temps √† l'audience d'assimiler
4. **Soyez pr√™ts aux questions** : Pr√©parez-vous √† expliquer les concepts math√©matiques en d√©tail
5. **Montrez votre passion** : Votre enthousiasme rendra la pr√©sentation plus engageante

### Points cl√©s √† souligner :
- **Le probl√®me** : Vanishing Gradient est le probl√®me central que LSTM r√©sout
- **L'innovation** : Les portes (gates) sont la cl√© de la solution
- **L'application** : L'√©tude de cas montre l'utilit√© pratique
- **L'avenir** : Les Transformers sont l'√©volution, mais LSTM reste pertinent

---

**Bonne pr√©sentation ! üéØ**
