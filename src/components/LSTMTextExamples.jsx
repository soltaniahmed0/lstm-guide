import React, { useState } from 'react'
import './LSTMTextExamples.css'

function LSTMTextExamples({ presentationMode = false }) {
  const [activeExample, setActiveExample] = useState(0)

  const examples = [
    {
      title: "Exemple 1 : Comprendre une Phrase Complexe",
      scenario: "Analyse de sentiment dans une phrase avec n√©gation",
      text: "Ce film n'est pas mauvais du tout",
      explanation: {
        step1: {
          title: "Tokenisation",
          content: "La phrase est d√©coup√©e en mots : ['Ce', 'film', \"n'est\", 'pas', 'mauvais', 'du', 'tout']"
        },
        step2: {
          title: "Traitement Mot par Mot",
          content: "L'LSTM traite chaque mot s√©quentiellement, en gardant le contexte des mots pr√©c√©dents"
        },
        step3: {
          title: "D√©tection de la N√©gation",
          content: "Quand LSTM voit 'n'est' et 'pas', le Forget Gate d√©cide de retenir cette information importante dans la Cell State"
        },
        step4: {
          title: "Interpr√©tation du Mot 'mauvais'",
          content: "Quand LSTM arrive √† 'mauvais' (mot n√©gatif), l'Input Gate combine cette information avec la n√©gation pr√©c√©dente"
        },
        step5: {
          title: "R√©sultat Final",
          content: "L'LSTM comprend que 'pas mauvais' = positif, gr√¢ce √† sa capacit√© de retenir la n√©gation sur plusieurs mots"
        }
      },
      comparison: {
        rnn: "Un RNN classique pourrait oublier la n√©gation avant d'arriver √† 'mauvais', et classer la phrase comme n√©gative",
        lstm: "L'LSTM retient la n√©gation gr√¢ce au Forget Gate et au Cell State, permettant une compr√©hension correcte"
      }
    },
    {
      title: "Exemple 2 : Traduction de Phrase Longue",
      scenario: "Traduire une phrase complexe de l'anglais vers le fran√ßais",
      text: "The cat that I saw yesterday in the park was very friendly",
      explanation: {
        step1: {
          title: "Encoder - Lecture de la Phrase",
          content: "L'encodeur LSTM lit la phrase anglaise mot par mot, en accumulant le sens dans son Cell State"
        },
        step2: {
          title: "Gestion des D√©pendances Longues",
          content: "Le mot 'cat' est li√© √† 'friendly' m√™me s'ils sont s√©par√©s par plusieurs mots. L'LSTM retient 'cat' gr√¢ce au Forget Gate"
        },
        step3: {
          title: "√âtat Final de l'Encodeur",
          content: "Apr√®s avoir lu toute la phrase, le Cell State contient : sujet='cat', action='was', qualit√©='friendly', lieu='park', temps='yesterday'"
        },
        step4: {
          title: "D√©codeur - G√©n√©ration",
          content: "Le d√©codeur LSTM commence avec l'√©tat de l'encodeur et g√©n√®re 'Le chat' en utilisant le contexte"
        },
        step5: {
          title: "Traduction Compl√®te",
          content: "Le d√©codeur g√©n√®re : 'Le chat que j'ai vu hier dans le parc √©tait tr√®s amical', en utilisant toutes les informations retenues"
        }
      },
      comparison: {
        rnn: "Un RNN pourrait oublier 'cat' avant d'arriver √† 'friendly', produisant une traduction incorrecte",
        lstm: "L'LSTM maintient l'information de 'cat' tout au long gr√¢ce au Cell State, permettant une traduction pr√©cise"
      }
    },
    {
      title: "Exemple 3 : Pr√©diction de S√©rie Temporelle",
      scenario: "Pr√©dire le prix d'une action en analysant l'historique",
      text: "Prix historiques : [100, 102, 105, 103, 108, 110, 107, 112, 115]",
      explanation: {
        step1: {
          title: "S√©quence d'Entr√©e",
          content: "L'LSTM re√ßoit une fen√™tre de 5 prix : [100, 102, 105, 103, 108] pour pr√©dire le 6√®me"
        },
        step2: {
          title: "D√©tection de Tendance",
          content: "En analysant la s√©quence, l'LSTM d√©tecte une tendance haussi√®re g√©n√©rale (100‚Üí108) malgr√© une petite baisse (105‚Üí103)"
        },
        step3: {
          title: "Forget Gate - Oublier les Anomalies",
          content: "Le Forget Gate d√©cide de donner moins de poids √† la baisse temporaire (103) car elle ne suit pas la tendance g√©n√©rale"
        },
        step4: {
          title: "Input Gate - Retenir la Tendance",
          content: "L'Input Gate stocke fortement l'information de la tendance haussi√®re dans le Cell State"
        },
        step5: {
          title: "Pr√©diction",
          content: "Bas√© sur la tendance retenue, l'LSTM pr√©dit 110 (ce qui est correct !), montrant sa capacit√© √† ignorer le bruit et suivre la tendance"
        }
      },
      comparison: {
        rnn: "Un RNN pourrait √™tre trop influenc√© par la derni√®re valeur (108) ou la baisse (103), produisant une pr√©diction moins pr√©cise",
        lstm: "L'LSTM utilise le Forget Gate pour filtrer le bruit et se concentrer sur la tendance √† long terme"
      }
    },
    {
      title: "Exemple 4 : G√©n√©ration de Texte Narratif",
      scenario: "G√©n√©rer la suite d'une histoire",
      text: "Il √©tait une fois un prince qui vivait dans un ch√¢teau magnifique",
      explanation: {
        step1: {
          title: "Contexte Initial",
          content: "L'LSTM traite 'Il √©tait une fois' et comprend qu'il s'agit d'un d√©but de conte de f√©es"
        },
        step2: {
          title: "Style Narratif",
          content: "Le Cell State stocke le style narratif (conte de f√©es) et le genre (fantastique)"
        },
        step3: {
          title: "Personnage Principal",
          content: "Quand l'LSTM voit 'prince', il retient cette information importante dans le Cell State pour l'utiliser plus tard"
        },
        step4: {
          title: "D√©tails du Contexte",
          content: "Les mots 'ch√¢teau' et 'magnifique' enrichissent le contexte, mais l'LSTM garde aussi l'information du prince"
        },
        step5: {
          title: "G√©n√©ration de la Suite",
          content: "L'LSTM g√©n√®re : '... qui cherchait une princesse'. Il utilise le contexte (conte de f√©es) et le personnage (prince) retenus dans le Cell State"
        }
      },
      comparison: {
        rnn: "Un RNN pourrait oublier qu'on parle d'un prince avant de g√©n√©rer la suite, produisant une incoh√©rence",
        lstm: "L'LSTM maintient l'information du prince gr√¢ce au Cell State, permettant une g√©n√©ration coh√©rente"
      }
    },
    {
      title: "Exemple 5 : Analyse de Conversation",
      scenario: "Comprendre le contexte dans une conversation",
      text: "A: 'As-tu vu mon chat ?' B: 'Non, je ne l'ai pas vu. Pourquoi ?' A: 'Il a disparu hier soir'",
      explanation: {
        step1: {
          title: "Premi√®re R√©plique",
          content: "L'LSTM encode la question sur le chat et retient cette information dans le Cell State"
        },
        step2: {
          title: "R√©ponse de B",
          content: "L'LSTM comprend que 'l' fait r√©f√©rence au chat de la premi√®re r√©plique, gr√¢ce au contexte retenu"
        },
        step3: {
          title: "Question de Suivi",
          content: "Quand B demande 'Pourquoi ?', l'LSTM comprend qu'il s'agit de la raison de la recherche du chat"
        },
        step4: {
          title: "R√©ponse Finale",
          content: "A r√©pond 'Il a disparu', et l'LSTM comprend que 'Il' = le chat mentionn√© au d√©but, gr√¢ce au contexte maintenu"
        },
        step5: {
          title: "Coh√©rence Conversationnelle",
          content: "L'LSTM maintient le contexte sur plusieurs tours de conversation, permettant de comprendre les r√©f√©rences (chat, il, l')"
        }
      },
      comparison: {
        rnn: "Un RNN pourrait oublier le sujet (chat) apr√®s quelques r√©pliques, ne comprenant plus les r√©f√©rences",
        lstm: "L'LSTM maintient le contexte conversationnel gr√¢ce au Cell State, permettant de suivre les r√©f√©rences sur plusieurs tours"
      }
    }
  ]

  return (
    <div className={`lstm-text-examples-container ${presentationMode ? 'presentation-mode' : ''}`}>
      {!presentationMode && (
        <>
          <h2>üìù Exemples Textuels - Comment LSTM Fonctionne</h2>
          <p className="intro-text">
            Exemples concrets montrant comment LSTM traite et comprend diff√©rents types de texte, avec explications d√©taill√©es √©tape par √©tape.
          </p>
        </>
      )}

      <div className="examples-tabs">
        {examples.map((example, index) => (
          <button
            key={index}
            className={`example-tab ${activeExample === index ? 'active' : ''}`}
            onClick={() => setActiveExample(index)}
          >
            Exemple {index + 1}
          </button>
        ))}
      </div>

      {examples[activeExample] && (
        <div className="example-content">
          <div className="example-header">
            <h3>{examples[activeExample].title}</h3>
            <div className="scenario-box">
              <strong>Sc√©nario :</strong> {examples[activeExample].scenario}
            </div>
            <div className="text-box">
              <strong>Texte :</strong> "{examples[activeExample].text}"
            </div>
          </div>

          <div className="explanation-flow">
            <h4>üîÑ Processus LSTM √âtape par √âtape :</h4>
            {Object.entries(examples[activeExample].explanation).map(([key, value], index) => (
              <div key={key} className="flow-step">
                <div className="step-header">
                  <div className="step-number">{index + 1}</div>
                  <h5>{value.title}</h5>
                </div>
                <div className="step-content">
                  <p>{value.content}</p>
                </div>
                {index < Object.keys(examples[activeExample].explanation).length - 1 && (
                  <div className="step-arrow">‚Üì</div>
                )}
              </div>
            ))}
          </div>

          <div className="comparison-box">
            <h4>‚öñÔ∏è Comparaison RNN vs LSTM :</h4>
            <div className="comparison-grid">
              <div className="comparison-item rnn">
                <div className="comparison-header">
                  <span className="icon">üîÑ</span>
                  <strong>RNN Classique</strong>
                </div>
                <p>{examples[activeExample].comparison.rnn}</p>
              </div>
              <div className="comparison-item lstm">
                <div className="comparison-header">
                  <span className="icon">‚úÖ</span>
                  <strong>LSTM</strong>
                </div>
                <p>{examples[activeExample].comparison.lstm}</p>
              </div>
            </div>
          </div>

          <div className="key-insights">
            <h4>üí° Points Cl√©s de cet Exemple :</h4>
            <ul>
              {activeExample === 0 && (
                <>
                  <li>L'LSTM doit retenir la n√©gation sur plusieurs mots</li>
                  <li>Le Forget Gate garde l'information importante (n√©gation)</li>
                  <li>L'Input Gate combine correctement les informations contradictoires</li>
                </>
              )}
              {activeExample === 1 && (
                <>
                  <li>L'LSTM g√®re des d√©pendances √† tr√®s long terme</li>
                  <li>Le Cell State maintient l'information du sujet sur toute la phrase</li>
                  <li>Le d√©codeur utilise le contexte complet pour g√©n√©rer la traduction</li>
                </>
              )}
              {activeExample === 2 && (
                <>
                  <li>L'LSTM filtre le bruit (valeurs aberrantes) gr√¢ce au Forget Gate</li>
                  <li>Le Cell State retient la tendance √† long terme</li>
                  <li>L'LSTM ignore les variations temporaires pour se concentrer sur le pattern global</li>
                </>
              )}
              {activeExample === 3 && (
                <>
                  <li>L'LSTM maintient le style et le genre du texte</li>
                  <li>Le Cell State retient les personnages et le contexte narratif</li>
                  <li>La g√©n√©ration reste coh√©rente gr√¢ce √† la m√©moire √† long terme</li>
                </>
              )}
              {activeExample === 4 && (
                <>
                  <li>L'LSTM maintient le contexte conversationnel sur plusieurs tours</li>
                  <li>Le Cell State retient les r√©f√©rences (chat, il, l')</li>
                  <li>L'LSTM comprend les pronoms et r√©f√©rences gr√¢ce au contexte retenu</li>
                </>
              )}
            </ul>
          </div>
        </div>
      )}
    </div>
  )
}

export default LSTMTextExamples

