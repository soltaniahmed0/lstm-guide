import React, { useState, useEffect } from 'react'
import './RealExamples.css'

function RealExamples({ presentationMode = false }) {
  const [selectedExample, setSelectedExample] = useState(presentationMode ? 'sentiment' : null)
  const [currentStep, setCurrentStep] = useState(0)
  const [isRunning, setIsRunning] = useState(false)
  const [autoPlayInterval, setAutoPlayInterval] = useState(null)

  const examples = {
    sentiment: {
      title: "Analyse de Sentiment",
      description: "D√©terminer si un texte est positif ou n√©gatif",
      input: "J'adore ce produit, il est vraiment excellent !",
      steps: [
        {
          step: "√âtape 1: Pr√©paration du Texte",
          subtitle: "Nettoyage et pr√©paration",
          description: "Nettoyer le texte en supprimant les caract√®res sp√©ciaux et normaliser",
          calculation: "Texte original ‚Üí Texte nettoy√©",
          input: "J'adore ce produit, il est vraiment excellent !",
          output: "j adore ce produit il est vraiment excellent",
          values: { original_length: 45, cleaned_length: 42 }
        },
        {
          step: "√âtape 2: Tokenisation",
          subtitle: "D√©coupage en mots",
          description: "Transformer le texte en liste de tokens (mots individuels)",
          calculation: "Texte ‚Üí Liste de tokens",
          input: "j adore ce produit il est vraiment excellent",
          output: "['j', 'adore', 'ce', 'produit', 'il', 'est', 'vraiment', 'excellent']",
          values: { num_tokens: 8 }
        },
        {
          step: "√âtape 3: Cr√©ation du Vocabulaire",
          subtitle: "Indexation des mots",
          description: "Cr√©er un dictionnaire qui associe chaque mot √† un nombre unique",
          calculation: "Mots ‚Üí Indices num√©riques",
          input: "['j', 'adore', 'ce', 'produit', 'il', 'est', 'vraiment', 'excellent']",
          output: "{'j': 1, 'adore': 2, 'ce': 3, 'produit': 4, 'il': 5, 'est': 6, 'vraiment': 7, 'excellent': 8}",
          values: { vocab_size: 8 }
        },
        {
          step: "√âtape 4: Conversion en Indices",
          subtitle: "Transformation num√©rique",
          description: "Convertir chaque token en son indice num√©rique",
          calculation: "Tokens ‚Üí Indices",
          input: "['j', 'adore', 'ce', 'produit', 'il', 'est', 'vraiment', 'excellent']",
          output: "[1, 2, 3, 4, 5, 6, 7, 8]",
          values: { sequence: [1, 2, 3, 4, 5, 6, 7, 8] }
        },
        {
          step: "√âtape 5: Embedding - Mot 'j'",
          subtitle: "Vecteur de repr√©sentation",
          description: "Convertir l'indice 1 en vecteur dense de 128 dimensions",
          calculation: "Embedding[1] = vecteur de 128 dimensions",
          input: "Indice: 1",
          output: "Vecteur: [0.12, -0.45, 0.78, ..., 0.23] (128 valeurs)",
          values: { embedding_dim: 128, word: "j" }
        },
        {
          step: "√âtape 6: Embedding - Mot 'adore'",
          subtitle: "Vecteur avec sentiment positif",
          description: "Le mot 'adore' a un embedding qui contient des informations sur le sentiment positif",
          calculation: "Embedding[2] = vecteur avec composantes sentimentales",
          input: "Indice: 2 (mot: 'adore')",
          output: "Vecteur: [0.45, 0.67, -0.12, ..., 0.89] (sentiment positif encod√©)",
          values: { embedding_dim: 128, word: "adore", sentiment_score: 0.75 }
        },
        {
          step: "√âtape 7: LSTM - Traitement du premier mot 'j'",
          subtitle: "Initialisation",
          description: "Traiter le premier mot avec h‚ÇÄ = 0 et C‚ÇÄ = 0",
          calculation: "h‚ÇÄ = [0, 0, ..., 0], C‚ÇÄ = [0, 0, ..., 0]",
          input: "Mot: 'j' (vecteur embedding)",
          output: "h‚ÇÅ calcul√©, C‚ÇÅ calcul√©",
          values: { h0: "zeros", c0: "zeros", word: "j" }
        },
        {
          step: "√âtape 8: LSTM - Forget Gate pour 'j'",
          subtitle: "Calcul f‚ÇÅ",
          description: "Calculer combien d'information garder de l'√©tat pr√©c√©dent (ici 0 car c'est le d√©but)",
          calculation: "f‚ÇÅ = œÉ(Wf ¬∑ [h‚ÇÄ, x‚ÇÅ] + bf) = œÉ(Wf ¬∑ [0, embedding('j')] + bf)",
          formula: "f‚Çú = œÉ(Wf ¬∑ [h‚Çú‚Çã‚ÇÅ, x‚Çú] + bf)\n\nO√π:\n‚Ä¢ œÉ(x) = 1/(1 + e‚ÅªÀ£) - fonction sigmo√Øde\n‚Ä¢ Wf = matrice de poids (apprise)\n‚Ä¢ bf = biais (appris)\n‚Ä¢ R√©sultat entre 0 et 1",
          input: "h‚ÇÄ = 0, x‚ÇÅ = embedding('j')",
          output: "f‚ÇÅ = 0.65 (garde 65% de l'√©tat pr√©c√©dent)",
          values: { ft: 0.65, calculation: "œÉ(-0.2) = 0.65", formula_detail: "Wf¬∑[0,embedding('j')] + bf = -0.2 ‚Üí œÉ(-0.2) = 1/(1+e^0.2) ‚âà 0.65" }
        },
        {
          step: "√âtape 9: LSTM - Input Gate pour 'j'",
          subtitle: "Calcul i‚ÇÅ",
          description: "D√©cider quelle nouvelle information stocker du mot 'j'",
          calculation: "i‚ÇÅ = œÉ(Wi ¬∑ [h‚ÇÄ, x‚ÇÅ] + bi) = œÉ(Wi ¬∑ [0, embedding('j')] + bi)",
          formula: "i‚Çú = œÉ(Wi ¬∑ [h‚Çú‚Çã‚ÇÅ, x‚Çú] + bi)\n\nO√π:\n‚Ä¢ œÉ = sigmo√Øde (0 √† 1)\n‚Ä¢ Wi = matrice de poids pour input gate\n‚Ä¢ bi = biais pour input gate\n‚Ä¢ i‚Çú ‚âà 1 ‚Üí stocke beaucoup, i‚Çú ‚âà 0 ‚Üí stocke peu",
          input: "h‚ÇÄ = 0, x‚ÇÅ = embedding('j')",
          output: "i‚ÇÅ = 0.72 (stocke 72% de l'information du mot 'j')",
          values: { it: 0.72, calculation: "œÉ(0.5) = 0.72", formula_detail: "Wi¬∑[0,embedding('j')] + bi = 0.5 ‚Üí œÉ(0.5) = 1/(1+e^-0.5) ‚âà 0.72" }
        },
        {
          step: "√âtape 10: LSTM - Candidate Values pour 'j'",
          subtitle: "Calcul CÃÉ‚ÇÅ",
          description: "Calculer les valeurs candidates pour le nouveau contenu",
          calculation: "CÃÉ‚ÇÅ = tanh(WC ¬∑ [h‚ÇÄ, x‚ÇÅ] + bC) = tanh(WC ¬∑ [0, embedding('j')] + bC)",
          formula: "CÃÉ‚Çú = tanh(WC ¬∑ [h‚Çú‚Çã‚ÇÅ, x‚Çú] + bC)\n\nO√π:\n‚Ä¢ tanh(x) = (eÀ£ - e‚ÅªÀ£)/(eÀ£ + e‚ÅªÀ£) - tangente hyperbolique\n‚Ä¢ WC = matrice de poids pour candidates\n‚Ä¢ bC = biais pour candidates\n‚Ä¢ R√©sultat entre -1 et 1",
          input: "h‚ÇÄ = 0, x‚ÇÅ = embedding('j')",
          output: "CÃÉ‚ÇÅ = 0.15 (nouvelle information √† ajouter)",
          values: { ctilde: 0.15, calculation: "tanh(0.15) = 0.15", formula_detail: "WC¬∑[0,embedding('j')] + bC = 0.15 ‚Üí tanh(0.15) ‚âà 0.15" }
        },
        {
          step: "√âtape 11: LSTM - Mise √† jour Cell State pour 'j'",
          subtitle: "Calcul C‚ÇÅ",
          description: "Mettre √† jour l'√©tat de la cellule en combinant forget et input",
          calculation: "C‚ÇÅ = f‚ÇÅ ‚äô C‚ÇÄ + i‚ÇÅ ‚äô CÃÉ‚ÇÅ = 0.65 √ó 0 + 0.72 √ó 0.15 = 0.108",
          formula: "C‚Çú = f‚Çú ‚äô C‚Çú‚Çã‚ÇÅ + i‚Çú ‚äô CÃÉ‚Çú\n\nO√π:\n‚Ä¢ ‚äô = multiplication √©l√©ment par √©l√©ment (Hadamard)\n‚Ä¢ f‚Çú ‚äô C‚Çú‚Çã‚ÇÅ = partie de l'ancien √©tat gard√©e\n‚Ä¢ i‚Çú ‚äô CÃÉ‚Çú = nouvelle information ajout√©e\n‚Ä¢ C‚Çú peut rester stable si f‚Çú ‚âà 1 et i‚Çú ‚âà 0",
          input: "f‚ÇÅ = 0.65, C‚ÇÄ = 0, i‚ÇÅ = 0.72, CÃÉ‚ÇÅ = 0.15",
          output: "C‚ÇÅ = 0.108 (premi√®re information stock√©e)",
          values: { ct: 0.108, calculation: "0.65 √ó 0 + 0.72 √ó 0.15 = 0.108", formula_detail: "C‚ÇÅ = (0.65 ‚äô [0]) + (0.72 ‚äô [0.15]) = [0] + [0.108] = [0.108]" }
        },
        {
          step: "√âtape 12: LSTM - Output Gate pour 'j'",
          subtitle: "Calcul o‚ÇÅ",
          description: "D√©cider quelle partie de C‚ÇÅ utiliser pour h‚ÇÅ",
          calculation: "o‚ÇÅ = œÉ(Wo ¬∑ [h‚ÇÄ, x‚ÇÅ] + bo) = œÉ(Wo ¬∑ [0, embedding('j')] + bo)",
          formula: "o‚Çú = œÉ(Wo ¬∑ [h‚Çú‚Çã‚ÇÅ, x‚Çú] + bo)\n\nO√π:\n‚Ä¢ œÉ = sigmo√Øde (0 √† 1)\n‚Ä¢ Wo = matrice de poids pour output gate\n‚Ä¢ bo = biais pour output gate\n‚Ä¢ o‚Çú filtre le Cell State pour la sortie",
          input: "h‚ÇÄ = 0, x‚ÇÅ = embedding('j')",
          output: "o‚ÇÅ = 0.68 (utilise 68% de C‚ÇÅ)",
          values: { ot: 0.68, calculation: "œÉ(0.3) = 0.68", formula_detail: "Wo¬∑[0,embedding('j')] + bo = 0.3 ‚Üí œÉ(0.3) ‚âà 0.68" }
        },
        {
          step: "√âtape 13: LSTM - Hidden State pour 'j'",
          subtitle: "Calcul h‚ÇÅ",
          description: "Calculer l'√©tat cach√© final apr√®s traitement de 'j'",
          calculation: "h‚ÇÅ = o‚ÇÅ ‚äô tanh(C‚ÇÅ) = 0.68 √ó tanh(0.108) = 0.68 √ó 0.107 = 0.073",
          formula: "h‚Çú = o‚Çú ‚äô tanh(C‚Çú)\n\nO√π:\n‚Ä¢ tanh(C‚Çú) = normalise le Cell State entre -1 et 1\n‚Ä¢ o‚Çú = filtre pour choisir quelle partie utiliser\n‚Ä¢ ‚äô = multiplication √©l√©ment par √©l√©ment\n‚Ä¢ h‚Çú est utilis√© pour pr√©dictions et prochaine √©tape",
          input: "o‚ÇÅ = 0.68, C‚ÇÅ = 0.108",
          output: "h‚ÇÅ = 0.073 (√©tat apr√®s 'j')",
          values: { ht: 0.073, calculation: "0.68 √ó 0.107 = 0.073", formula_detail: "h‚ÇÅ = 0.68 ‚äô tanh(0.108) = 0.68 √ó 0.107 ‚âà 0.073" }
        },
        {
          step: "√âtape 14: LSTM - Traitement du mot 'adore'",
          subtitle: "Mot avec sentiment positif fort",
          description: "Traiter le mot 'adore' qui a un sentiment tr√®s positif",
          calculation: "Utiliser h‚ÇÅ et embedding('adore') comme entr√©es",
          input: "h‚ÇÅ = 0.073, x‚ÇÇ = embedding('adore')",
          output: "Pr√©paration pour calculer h‚ÇÇ",
          values: { previous_h: 0.073, word: "adore" }
        },
        {
          step: "√âtape 15: LSTM - Forget Gate pour 'adore'",
          subtitle: "Calcul f‚ÇÇ",
          description: "D√©cider combien garder de l'information pr√©c√©dente (mot 'j')",
          calculation: "f‚ÇÇ = œÉ(Wf ¬∑ [h‚ÇÅ, x‚ÇÇ] + bf) = œÉ(Wf ¬∑ [0.073, embedding('adore')] + bf)",
          input: "h‚ÇÅ = 0.073, x‚ÇÇ = embedding('adore')",
          output: "f‚ÇÇ = 0.82 (garde 82% de l'information pr√©c√©dente)",
          values: { ft: 0.82, calculation: "œÉ(1.2) = 0.82" }
        },
        {
          step: "√âtape 16: LSTM - Input Gate pour 'adore'",
          subtitle: "Calcul i‚ÇÇ",
          description: "Le mot 'adore' est important, on veut le stocker",
          calculation: "i‚ÇÇ = œÉ(Wi ¬∑ [h‚ÇÅ, x‚ÇÇ] + bi) = œÉ(Wi ¬∑ [0.073, embedding('adore')] + bi)",
          input: "h‚ÇÅ = 0.073, x‚ÇÇ = embedding('adore')",
          output: "i‚ÇÇ = 0.91 (stocke 91% - mot tr√®s important)",
          values: { it: 0.91, calculation: "œÉ(2.1) = 0.91" }
        },
        {
          step: "√âtape 17: LSTM - Candidate pour 'adore'",
          subtitle: "Calcul CÃÉ‚ÇÇ",
          description: "Calculer la nouvelle information positive du mot 'adore'",
          calculation: "CÃÉ‚ÇÇ = tanh(WC ¬∑ [h‚ÇÅ, x‚ÇÇ] + bC) = tanh(WC ¬∑ [0.073, embedding('adore')] + bC)",
          input: "h‚ÇÅ = 0.073, x‚ÇÇ = embedding('adore')",
          output: "CÃÉ‚ÇÇ = 0.68 (fort sentiment positif)",
          values: { ctilde: 0.68, calculation: "tanh(0.85) = 0.68" }
        },
        {
          step: "√âtape 18: LSTM - Mise √† jour Cell State pour 'adore'",
          subtitle: "Calcul C‚ÇÇ",
          description: "Combiner l'ancien √©tat (de 'j') avec le nouveau (de 'adore')",
          calculation: "C‚ÇÇ = f‚ÇÇ ‚äô C‚ÇÅ + i‚ÇÇ ‚äô CÃÉ‚ÇÇ = 0.82 √ó 0.108 + 0.91 √ó 0.68 = 0.089 + 0.619 = 0.708",
          input: "f‚ÇÇ = 0.82, C‚ÇÅ = 0.108, i‚ÇÇ = 0.91, CÃÉ‚ÇÇ = 0.68",
          output: "C‚ÇÇ = 0.708 (sentiment positif accumul√©)",
          values: { ct: 0.708, calculation: "0.82 √ó 0.108 + 0.91 √ó 0.68 = 0.708" }
        },
        {
          step: "√âtape 19: LSTM - Traitement des mots suivants",
          subtitle: "Accumulation continue",
          description: "Traiter 'ce', 'produit', 'il', 'est', 'vraiment' de mani√®re similaire",
          calculation: "R√©p√©ter les √©tapes 14-18 pour chaque mot",
          input: "Mots: 'ce', 'produit', 'il', 'est', 'vraiment'",
          output: "C‚Çá = 1.25 (sentiment positif continue d'augmenter)",
          values: { ct: 1.25, words_processed: 5 }
        },
        {
          step: "√âtape 20: LSTM - Traitement du mot 'excellent'",
          subtitle: "Mot cl√© tr√®s positif",
          description: "Le mot 'excellent' renforce fortement le sentiment positif",
          calculation: "Traitement avec h‚Çá et embedding('excellent')",
          input: "h‚Çá = 0.85, x‚Çà = embedding('excellent')",
          output: "Pr√©paration pour calcul final",
          values: { previous_h: 0.85, word: "excellent" }
        },
        {
          step: "√âtape 21: LSTM - Forget Gate pour 'excellent'",
          subtitle: "Calcul f‚Çà",
          description: "Garder l'information positive accumul√©e",
          calculation: "f‚Çà = œÉ(Wf ¬∑ [h‚Çá, x‚Çà] + bf) = œÉ(Wf ¬∑ [0.85, embedding('excellent')] + bf)",
          input: "h‚Çá = 0.85, x‚Çà = embedding('excellent')",
          output: "f‚Çà = 0.88 (garde 88% du sentiment positif accumul√©)",
          values: { ft: 0.88, calculation: "œÉ(1.8) = 0.88" }
        },
        {
          step: "√âtape 22: LSTM - Input Gate pour 'excellent'",
          subtitle: "Calcul i‚Çà",
          description: "Le mot 'excellent' est tr√®s important, on veut tout stocker",
          calculation: "i‚Çà = œÉ(Wi ¬∑ [h‚Çá, x‚Çà] + bi) = œÉ(Wi ¬∑ [0.85, embedding('excellent')] + bi)",
          input: "h‚Çá = 0.85, x‚Çà = embedding('excellent')",
          output: "i‚Çà = 0.95 (stocke 95% - mot tr√®s important)",
          values: { it: 0.95, calculation: "œÉ(2.8) = 0.95" }
        },
        {
          step: "√âtape 23: LSTM - Candidate pour 'excellent'",
          subtitle: "Calcul CÃÉ‚Çà",
          description: "Calculer la valeur tr√®s positive du mot 'excellent'",
          calculation: "CÃÉ‚Çà = tanh(WC ¬∑ [h‚Çá, x‚Çà] + bC) = tanh(WC ¬∑ [0.85, embedding('excellent')] + bC)",
          input: "h‚Çá = 0.85, x‚Çà = embedding('excellent')",
          output: "CÃÉ‚Çà = 0.92 (sentiment tr√®s positif)",
          values: { ctilde: 0.92, calculation: "tanh(1.5) = 0.92" }
        },
        {
          step: "√âtape 24: LSTM - Mise √† jour Cell State finale",
          subtitle: "Calcul C‚Çà",
          description: "Combiner tout le sentiment positif accumul√©",
          calculation: "C‚Çà = f‚Çà ‚äô C‚Çá + i‚Çà ‚äô CÃÉ‚Çà = 0.88 √ó 1.25 + 0.95 √ó 0.92 = 1.10 + 0.874 = 1.974",
          input: "f‚Çà = 0.88, C‚Çá = 1.25, i‚Çà = 0.95, CÃÉ‚Çà = 0.92",
          output: "C‚Çà = 1.974 (sentiment tr√®s positif final)",
          values: { ct: 1.974, calculation: "0.88 √ó 1.25 + 0.95 √ó 0.92 = 1.974" }
        },
        {
          step: "√âtape 25: LSTM - Output Gate final",
          subtitle: "Calcul o‚Çà",
          description: "Utiliser l'information pour la pr√©diction",
          calculation: "o‚Çà = œÉ(Wo ¬∑ [h‚Çá, x‚Çà] + bo) = œÉ(Wo ¬∑ [0.85, embedding('excellent')] + bo)",
          input: "h‚Çá = 0.85, x‚Çà = embedding('excellent')",
          output: "o‚Çà = 0.90 (utilise 90% de C‚Çà)",
          values: { ot: 0.90, calculation: "œÉ(2.0) = 0.90" }
        },
        {
          step: "√âtape 26: LSTM - Hidden State final",
          subtitle: "Calcul h‚Çà",
          description: "√âtat cach√© final contenant toute l'information du texte",
          calculation: "h‚Çà = o‚Çà ‚äô tanh(C‚Çà) = 0.90 √ó tanh(1.974) = 0.90 √ó 0.96 = 0.864",
          input: "o‚Çà = 0.90, C‚Çà = 1.974",
          output: "h‚Çà = 0.864 (repr√©sentation finale du texte)",
          values: { ht: 0.864, calculation: "0.90 √ó 0.96 = 0.864" }
        },
        {
          step: "√âtape 27: Couche Dense",
          subtitle: "Classification",
          description: "Passer h‚Çà √† travers une couche dense pour la classification",
          calculation: "y = W ¬∑ h‚Çà + b",
          input: "h‚Çà = 0.864 (vecteur de 256 dimensions)",
          output: "y = [0.94, 0.06] (probabilit√©s: POSITIF=94%, N√âGATIF=6%)",
          values: { positive_prob: 0.94, negative_prob: 0.06 }
        },
        {
          step: "√âtape 28: Pr√©diction Finale",
          subtitle: "R√©sultat",
          description: "S√©lectionner la classe avec la plus haute probabilit√©",
          calculation: "argmax(y) = POSITIF",
          input: "Probabilit√©s: [0.94, 0.06]",
          output: "Sentiment: POSITIF (confiance: 94%)",
          values: { prediction: "POSITIF", confidence: 0.94 }
        }
      ]
    },
    prediction: {
      title: "Pr√©diction de S√©rie Temporelle",
      description: "Pr√©dire le prix d'une action bas√© sur l'historique",
      input: "Prix historiques: [100, 102, 105, 103, 108, 110, 107]",
      steps: [
        {
          step: "√âtape 1: Pr√©paration des Donn√©es",
          subtitle: "Collecte des prix",
          description: "Collecter les prix historiques sur 7 jours",
          calculation: "Prix bruts",
          input: "Prix observ√©s",
          output: "[100, 102, 105, 103, 108, 110, 107]",
          values: { min: 100, max: 110, mean: 105 }
        },
        {
          step: "√âtape 2: Calcul des Statistiques",
          subtitle: "Min, Max, Moyenne",
          description: "Calculer les statistiques pour la normalisation",
          calculation: "min = 100, max = 110, mean = 105",
          input: "[100, 102, 105, 103, 108, 110, 107]",
          output: "min=100, max=110, mean=105",
          values: { min: 100, max: 110, mean: 105 }
        },
        {
          step: "√âtape 3: Normalisation Min-Max",
          subtitle: "Mise √† l'√©chelle entre 0 et 1",
          description: "Normaliser chaque prix: (prix - min) / (max - min)",
          calculation: "normalized = (prix - 100) / (110 - 100)",
          input: "Prix: 100, 102, 105, 103, 108, 110, 107",
          output: "[0.0, 0.2, 0.5, 0.3, 0.8, 1.0, 0.7]",
          values: { normalized: [0.0, 0.2, 0.5, 0.3, 0.8, 1.0, 0.7] }
        },
        {
          step: "√âtape 4: Cr√©ation des S√©quences",
          subtitle: "Window de 5 pas",
          description: "Cr√©er des s√©quences de 5 prix pour pr√©dire le 6√®me",
          calculation: "S√©quence: [prix_t-4, prix_t-3, prix_t-2, prix_t-1, prix_t] ‚Üí Target: prix_t+1",
          input: "Donn√©es normalis√©es",
          output: "Seq1: [0.0, 0.2, 0.5, 0.3, 0.8] ‚Üí Target: 1.0",
          values: { sequence_length: 5, num_sequences: 2 }
        },
        {
          step: "√âtape 5: LSTM - Traitement S√©quence 1",
          subtitle: "Premi√®re s√©quence",
          description: "Traiter la premi√®re s√©quence [0.0, 0.2, 0.5, 0.3, 0.8]",
          calculation: "Entr√©e: 5 valeurs normalis√©es",
          input: "[0.0, 0.2, 0.5, 0.3, 0.8]",
          output: "h‚ÇÖ calcul√©",
          values: { sequence: [0.0, 0.2, 0.5, 0.3, 0.8] }
        },
        {
          step: "√âtape 6: LSTM - Forget Gate (t=1)",
          subtitle: "Premier prix",
          description: "Traiter le premier prix normalis√© (0.0)",
          calculation: "f‚ÇÅ = œÉ(Wf ¬∑ [h‚ÇÄ, x‚ÇÅ] + bf) = œÉ(Wf ¬∑ [0, 0.0] + bf)",
          input: "h‚ÇÄ = 0, x‚ÇÅ = 0.0",
          output: "f‚ÇÅ = 0.70 (garde 70% de l'√©tat initial)",
          values: { ft: 0.70, calculation: "œÉ(-0.4) = 0.70" }
        },
        {
          step: "√âtape 7: LSTM - Input Gate (t=1)",
          subtitle: "Stockage premier prix",
          description: "D√©cider combien stocker du premier prix",
          calculation: "i‚ÇÅ = œÉ(Wi ¬∑ [h‚ÇÄ, x‚ÇÅ] + bi) = œÉ(Wi ¬∑ [0, 0.0] + bi)",
          input: "h‚ÇÄ = 0, x‚ÇÅ = 0.0",
          output: "i‚ÇÅ = 0.65 (stocke 65% de x‚ÇÅ)",
          values: { it: 0.65, calculation: "œÉ(-0.6) = 0.65" }
        },
        {
          step: "√âtape 8: LSTM - Cell State (t=1)",
          subtitle: "C‚ÇÅ apr√®s premier prix",
          description: "Mettre √† jour l'√©tat apr√®s le premier prix",
          calculation: "C‚ÇÅ = f‚ÇÅ ‚äô C‚ÇÄ + i‚ÇÅ ‚äô CÃÉ‚ÇÅ = 0.70 √ó 0 + 0.65 √ó 0.05 = 0.033",
          input: "f‚ÇÅ = 0.70, C‚ÇÄ = 0, i‚ÇÅ = 0.65, CÃÉ‚ÇÅ = 0.05",
          output: "C‚ÇÅ = 0.033",
          values: { ct: 0.033, calculation: "0.70 √ó 0 + 0.65 √ó 0.05 = 0.033" }
        },
        {
          step: "√âtape 9: LSTM - Traitement Prix 2 (t=2)",
          subtitle: "Prix normalis√© 0.2",
          description: "Traiter le deuxi√®me prix (0.2) avec le contexte du premier",
          calculation: "Utiliser h‚ÇÅ et x‚ÇÇ = 0.2",
          input: "h‚ÇÅ = 0.03, x‚ÇÇ = 0.2",
          output: "Calcul de f‚ÇÇ, i‚ÇÇ, C‚ÇÇ",
          values: { previous_h: 0.03, current_price: 0.2 }
        },
        {
          step: "√âtape 10: LSTM - Forget Gate (t=2)",
          subtitle: "Calcul f‚ÇÇ",
          description: "D√©cider combien garder de l'information du prix pr√©c√©dent",
          calculation: "f‚ÇÇ = œÉ(Wf ¬∑ [h‚ÇÅ, x‚ÇÇ] + bf) = œÉ(Wf ¬∑ [0.03, 0.2] + bf)",
          input: "h‚ÇÅ = 0.03, x‚ÇÇ = 0.2",
          output: "f‚ÇÇ = 0.78 (garde 78% de C‚ÇÅ)",
          values: { ft: 0.78, calculation: "œÉ(1.1) = 0.78" }
        },
        {
          step: "√âtape 11: LSTM - Input Gate (t=2)",
          subtitle: "Calcul i‚ÇÇ",
          description: "D√©cider combien stocker du nouveau prix",
          calculation: "i‚ÇÇ = œÉ(Wi ¬∑ [h‚ÇÅ, x‚ÇÇ] + bi) = œÉ(Wi ¬∑ [0.03, 0.2] + bi)",
          input: "h‚ÇÅ = 0.03, x‚ÇÇ = 0.2",
          output: "i‚ÇÇ = 0.72 (stocke 72% de x‚ÇÇ)",
          values: { it: 0.72, calculation: "œÉ(0.9) = 0.72" }
        },
        {
          step: "√âtape 12: LSTM - Cell State (t=2)",
          subtitle: "Calcul C‚ÇÇ",
          description: "Mettre √† jour avec les deux premiers prix",
          calculation: "C‚ÇÇ = f‚ÇÇ ‚äô C‚ÇÅ + i‚ÇÇ ‚äô CÃÉ‚ÇÇ = 0.78 √ó 0.033 + 0.72 √ó 0.18 = 0.026 + 0.130 = 0.156",
          input: "f‚ÇÇ = 0.78, C‚ÇÅ = 0.033, i‚ÇÇ = 0.72, CÃÉ‚ÇÇ = 0.18",
          output: "C‚ÇÇ = 0.156 (tendance en cours de formation)",
          values: { ct: 0.156, calculation: "0.78 √ó 0.033 + 0.72 √ó 0.18 = 0.156" }
        },
        {
          step: "√âtape 13: LSTM - Traitement Prix 3, 4, 5",
          subtitle: "Accumulation de la tendance",
          description: "Traiter les prix suivants (0.5, 0.3, 0.8) de mani√®re similaire",
          calculation: "R√©p√©ter les √©tapes pour chaque prix",
          input: "Prix: 0.5, 0.3, 0.8",
          output: "C‚ÇÖ = 0.65 (tendance haussi√®re d√©tect√©e)",
          values: { ct: 0.65, trend: "haussier" }
        },
        {
          step: "√âtape 14: LSTM - Output Gate Final",
          subtitle: "Calcul o‚ÇÖ",
          description: "G√©n√©rer la sortie pour la pr√©diction",
          calculation: "o‚ÇÖ = œÉ(Wo ¬∑ [h‚ÇÑ, x‚ÇÖ] + bo) = œÉ(Wo ¬∑ [0.55, 0.8] + bo)",
          input: "h‚ÇÑ = 0.55, x‚ÇÖ = 0.8",
          output: "o‚ÇÖ = 0.88 (utilise 88% de C‚ÇÖ)",
          values: { ot: 0.88, calculation: "œÉ(1.8) = 0.88" }
        },
        {
          step: "√âtape 15: LSTM - Hidden State Final",
          subtitle: "Calcul h‚ÇÖ",
          description: "√âtat final apr√®s traitement de la s√©quence",
          calculation: "h‚ÇÖ = o‚ÇÖ ‚äô tanh(C‚ÇÖ) = 0.88 √ó tanh(0.65) = 0.88 √ó 0.57 = 0.50",
          input: "o‚ÇÖ = 0.88, C‚ÇÖ = 0.65",
          output: "h‚ÇÖ = 0.50 (repr√©sentation de la tendance)",
          values: { ht: 0.50, calculation: "0.88 √ó 0.57 = 0.50" }
        },
        {
          step: "√âtape 16: Couche Dense",
          subtitle: "Pr√©diction",
          description: "Transformer h‚ÇÖ en pr√©diction de prix",
          calculation: "prix_pr√©dit = W ¬∑ h‚ÇÖ + b",
          input: "h‚ÇÖ = 0.50",
          output: "prix_normalis√©_pr√©dit = 0.92",
          values: { predicted_normalized: 0.92 }
        },
        {
          step: "√âtape 17: D√©normalisation",
          subtitle: "Conversion en prix r√©el",
          description: "Convertir le prix normalis√© en prix r√©el",
          calculation: "prix_r√©el = prix_normalis√© √ó (max - min) + min = 0.92 √ó (110 - 100) + 100 = 0.92 √ó 10 + 100 = 109.2",
          input: "prix_normalis√© = 0.92, min = 100, max = 110",
          output: "Prix pr√©dit: 109.2",
          values: { predicted_price: 109.2, calculation: "0.92 √ó 10 + 100 = 109.2" }
        },
        {
          step: "√âtape 18: R√©sultat Final",
          subtitle: "Pr√©diction compl√®te",
          description: "Le mod√®le pr√©dit que le prochain prix sera 109.2",
          calculation: "Bas√© sur la tendance haussi√®re d√©tect√©e",
          input: "S√©quence: [0.0, 0.2, 0.5, 0.3, 0.8]",
          output: "Prix pr√©dit pour t+1: 109.2 (tendance haussi√®re)",
          values: { final_prediction: 109.2, confidence: 0.85 }
        }
      ]
    },
    translation: {
      title: "Traduction Automatique",
      description: "Traduire 'Hello, how are you?' en fran√ßais",
      input: "Hello, how are you?",
      steps: [
        {
          step: "√âtape 1: Pr√©paration du Texte Source",
          subtitle: "Nettoyage",
          description: "Nettoyer et pr√©parer le texte anglais",
          calculation: "Texte ‚Üí Texte nettoy√©",
          input: "Hello, how are you?",
          output: "hello how are you",
          values: { original_length: 18, cleaned_length: 16 }
        },
        {
          step: "√âtape 2: Tokenisation Source",
          subtitle: "D√©coupage en mots",
          description: "Transformer en liste de tokens",
          calculation: "Texte ‚Üí Tokens",
          input: "hello how are you",
          output: "['hello', 'how', 'are', 'you']",
          values: { num_tokens: 4 }
        },
        {
          step: "√âtape 3: Encodage - Embedding",
          subtitle: "Vecteurs de mots",
          description: "Convertir chaque mot anglais en vecteur",
          calculation: "Mots ‚Üí Vecteurs de 256 dimensions",
          input: "['hello', 'how', 'are', 'you']",
          output: "4 vecteurs de 256 dimensions",
          values: { embedding_dim: 256, num_words: 4 }
        },
        {
          step: "√âtape 4: LSTM Encoder - Mot 'hello'",
          subtitle: "Premier mot",
          description: "Encoder le premier mot avec h‚ÇÄ = 0, C‚ÇÄ = 0",
          calculation: "h‚ÇÄ = 0, C‚ÇÄ = 0, x‚ÇÅ = embedding('hello')",
          input: "Mot: 'hello'",
          output: "h‚ÇÅ, C‚ÇÅ calcul√©s",
          values: { word: "hello" }
        },
        {
          step: "√âtape 5: LSTM Encoder - Forget Gate (t=1)",
          subtitle: "Calcul f‚ÇÅ",
          description: "Calculer le forget gate pour 'hello'",
          calculation: "f‚ÇÅ = œÉ(Wf ¬∑ [h‚ÇÄ, x‚ÇÅ] + bf) = œÉ(Wf ¬∑ [0, embedding('hello')] + bf)",
          input: "h‚ÇÄ = 0, x‚ÇÅ = embedding('hello')",
          output: "f‚ÇÅ = 0.75 (garde 75%)",
          values: { ft: 0.75 }
        },
        {
          step: "√âtape 6: LSTM Encoder - Input Gate (t=1)",
          subtitle: "Calcul i‚ÇÅ",
          description: "Stockage de 'hello'",
          calculation: "i‚ÇÅ = œÉ(Wi ¬∑ [h‚ÇÄ, x‚ÇÅ] + bi)",
          input: "h‚ÇÄ = 0, x‚ÇÅ = embedding('hello')",
          output: "i‚ÇÅ = 0.80 (stocke 80%)",
          values: { it: 0.80 }
        },
        {
          step: "√âtape 7: LSTM Encoder - Cell State (t=1)",
          subtitle: "Calcul C‚ÇÅ",
          description: "Mettre √† jour l'√©tat apr√®s 'hello'",
          calculation: "C‚ÇÅ = f‚ÇÅ ‚äô C‚ÇÄ + i‚ÇÅ ‚äô CÃÉ‚ÇÅ = 0.75 √ó 0 + 0.80 √ó 0.25 = 0.20",
          input: "f‚ÇÅ = 0.75, C‚ÇÄ = 0, i‚ÇÅ = 0.80, CÃÉ‚ÇÅ = 0.25",
          output: "C‚ÇÅ = 0.20 (information 'hello' encod√©e)",
          values: { ct: 0.20, calculation: "0.75 √ó 0 + 0.80 √ó 0.25 = 0.20" }
        },
        {
          step: "√âtape 8: LSTM Encoder - Mots Suivants",
          subtitle: "Encodage complet",
          description: "Encoder 'how', 'are', 'you' de mani√®re similaire",
          calculation: "R√©p√©ter pour chaque mot",
          input: "Mots: 'how', 'are', 'you'",
          output: "C‚ÇÑ = 1.85 (repr√©sentation compl√®te de la phrase)",
          values: { ct: 1.85, words_encoded: 4 }
        },
        {
          step: "√âtape 9: LSTM Encoder - √âtat Final",
          subtitle: "h‚ÇÑ et C‚ÇÑ",
          description: "√âtat final de l'encodeur contenant toute la phrase",
          calculation: "h‚ÇÑ = 0.78, C‚ÇÑ = 1.85",
          input: "Apr√®s encodage de tous les mots",
          output: "h‚ÇÑ = 0.78, C‚ÇÑ = 1.85 (contexte complet encod√©)",
          values: { h_final: 0.78, c_final: 1.85 }
        },
        {
          step: "√âtape 10: LSTM Decoder - Initialisation",
          subtitle: "D√©but de la traduction",
          description: "Initialiser le d√©codeur avec l'√©tat final de l'encodeur",
          calculation: "h_decoder‚ÇÄ = h_encoder‚ÇÑ = 0.78, C_decoder‚ÇÄ = C_encoder‚ÇÑ = 1.85",
          input: "√âtat encodeur final",
          output: "D√©codeur initialis√© avec le contexte",
          values: { h_decoder: 0.78, c_decoder: 1.85 }
        },
        {
          step: "√âtape 11: LSTM Decoder - G√©n√©ration 'Bonjour'",
          subtitle: "Premier mot fran√ßais",
          description: "G√©n√©rer le premier mot de la traduction",
          calculation: "Utiliser h_decoder‚ÇÄ pour pr√©dire le premier mot",
          input: "h_decoder‚ÇÄ = 0.78, contexte = phrase anglaise",
          output: "Mot pr√©dit: 'Bonjour' (probabilit√©: 0.95)",
          values: { word: "Bonjour", probability: 0.95 }
        },
        {
          step: "√âtape 12: LSTM Decoder - Forget Gate (t=1)",
          subtitle: "Calcul f‚ÇÅ",
          description: "D√©cider combien garder du contexte encod√©",
          calculation: "f‚ÇÅ = œÉ(Wf ¬∑ [h_decoder‚ÇÄ, embedding('Bonjour')] + bf)",
          input: "h_decoder‚ÇÄ = 0.78, x‚ÇÅ = embedding('Bonjour')",
          output: "f‚ÇÅ = 0.85 (garde 85% du contexte)",
          values: { ft: 0.85 }
        },
        {
          step: "√âtape 13: LSTM Decoder - Input Gate (t=1)",
          subtitle: "Calcul i‚ÇÅ",
          description: "Int√©grer le mot g√©n√©r√© 'Bonjour'",
          calculation: "i‚ÇÅ = œÉ(Wi ¬∑ [h_decoder‚ÇÄ, embedding('Bonjour')] + bi)",
          input: "h_decoder‚ÇÄ = 0.78, x‚ÇÅ = embedding('Bonjour')",
          output: "i‚ÇÅ = 0.82 (int√®gre 82%)",
          values: { it: 0.82 }
        },
        {
          step: "√âtape 14: LSTM Decoder - Cell State (t=1)",
          subtitle: "Calcul C‚ÇÅ",
          description: "Mettre √† jour avec 'Bonjour'",
          calculation: "C‚ÇÅ = f‚ÇÅ ‚äô C_decoder‚ÇÄ + i‚ÇÅ ‚äô CÃÉ‚ÇÅ = 0.85 √ó 1.85 + 0.82 √ó 0.30 = 1.57 + 0.25 = 1.82",
          input: "f‚ÇÅ = 0.85, C_decoder‚ÇÄ = 1.85, i‚ÇÅ = 0.82, CÃÉ‚ÇÅ = 0.30",
          output: "C‚ÇÅ = 1.82 (contexte + 'Bonjour')",
          values: { ct: 1.82, calculation: "0.85 √ó 1.85 + 0.82 √ó 0.30 = 1.82" }
        },
        {
          step: "√âtape 15: LSTM Decoder - G√©n√©ration 'comment'",
          subtitle: "Deuxi√®me mot",
          description: "G√©n√©rer le deuxi√®me mot bas√© sur le contexte",
          calculation: "Utiliser h‚ÇÅ pour pr√©dire le prochain mot",
          input: "h‚ÇÅ = 0.72, contexte = 'Hello' + 'Bonjour'",
          output: "Mot pr√©dit: 'comment' (probabilit√©: 0.92)",
          values: { word: "comment", probability: 0.92 }
        },
        {
          step: "√âtape 16: LSTM Decoder - G√©n√©ration 'allez-vous'",
          subtitle: "Mots suivants",
          description: "G√©n√©rer les mots restants",
          calculation: "Continuer la g√©n√©ration",
          input: "Contexte accumul√©",
          output: "Mots: 'allez-vous' (probabilit√©: 0.88)",
          values: { words: "allez-vous", probability: 0.88 }
        },
        {
          step: "√âtape 17: Traduction Compl√®te",
          subtitle: "R√©sultat final",
          description: "Assembler tous les mots g√©n√©r√©s",
          calculation: "Concat√©nation des mots",
          input: "Mots g√©n√©r√©s: 'Bonjour', 'comment', 'allez-vous'",
          output: "Traduction: 'Bonjour, comment allez-vous ?'",
          values: { translation: "Bonjour, comment allez-vous ?", confidence: 0.92 }
        }
      ]
    },
    textGeneration: {
      title: "G√©n√©ration de Texte",
      description: "G√©n√©rer la suite d'un texte",
      input: "Il √©tait une fois",
      steps: [
        {
          step: "√âtape 1: Tokenisation",
          subtitle: "D√©coupage",
          description: "Transformer le texte en tokens",
          calculation: "Texte ‚Üí Tokens",
          input: "Il √©tait une fois",
          output: "['Il', '√©tait', 'une', 'fois']",
          values: { num_tokens: 4 }
        },
        {
          step: "√âtape 2: Embedding",
          subtitle: "Vecteurs",
          description: "Convertir en vecteurs",
          calculation: "Mots ‚Üí Vecteurs",
          input: "['Il', '√©tait', 'une', 'fois']",
          output: "4 vecteurs de 256 dimensions",
          values: { embedding_dim: 256 }
        },
        {
          step: "√âtape 3: LSTM - Traitement 'Il'",
          subtitle: "Premier mot",
          description: "Traiter le premier mot",
          calculation: "h‚ÇÄ = 0, C‚ÇÄ = 0, x‚ÇÅ = embedding('Il')",
          input: "Mot: 'Il'",
          output: "h‚ÇÅ, C‚ÇÅ calcul√©s",
          values: { word: "Il" }
        },
        {
          step: "√âtape 4: LSTM - Forget Gate (t=1)",
          subtitle: "Calcul f‚ÇÅ",
          description: "Forget gate pour 'Il'",
          calculation: "f‚ÇÅ = œÉ(Wf ¬∑ [h‚ÇÄ, x‚ÇÅ] + bf)",
          input: "h‚ÇÄ = 0, x‚ÇÅ = embedding('Il')",
          output: "f‚ÇÅ = 0.72",
          values: { ft: 0.72 }
        },
        {
          step: "√âtape 5: LSTM - Input Gate (t=1)",
          subtitle: "Calcul i‚ÇÅ",
          description: "Input gate pour 'Il'",
          calculation: "i‚ÇÅ = œÉ(Wi ¬∑ [h‚ÇÄ, x‚ÇÅ] + bi)",
          input: "h‚ÇÄ = 0, x‚ÇÅ = embedding('Il')",
          output: "i‚ÇÅ = 0.75",
          values: { it: 0.75 }
        },
        {
          step: "√âtape 6: LSTM - Cell State (t=1)",
          subtitle: "Calcul C‚ÇÅ",
          description: "Mise √† jour apr√®s 'Il'",
          calculation: "C‚ÇÅ = f‚ÇÅ ‚äô C‚ÇÄ + i‚ÇÅ ‚äô CÃÉ‚ÇÅ = 0.72 √ó 0 + 0.75 √ó 0.20 = 0.15",
          input: "f‚ÇÅ = 0.72, C‚ÇÄ = 0, i‚ÇÅ = 0.75, CÃÉ‚ÇÅ = 0.20",
          output: "C‚ÇÅ = 0.15",
          values: { ct: 0.15, calculation: "0.72 √ó 0 + 0.75 √ó 0.20 = 0.15" }
        },
        {
          step: "√âtape 7: LSTM - Traitement Mots Suivants",
          subtitle: "Accumulation",
          description: "Traiter '√©tait', 'une', 'fois'",
          calculation: "R√©p√©ter pour chaque mot",
          input: "Mots: '√©tait', 'une', 'fois'",
          output: "C‚ÇÑ = 1.25 (contexte narratif accumul√©)",
          values: { ct: 1.25, style: "narratif" }
        },
        {
          step: "√âtape 8: LSTM - Output Gate Final",
          subtitle: "Calcul o‚ÇÑ",
          description: "Output gate pour g√©n√©ration",
          calculation: "o‚ÇÑ = œÉ(Wo ¬∑ [h‚ÇÉ, x‚ÇÑ] + bo)",
          input: "h‚ÇÉ = 0.68, x‚ÇÑ = embedding('fois')",
          output: "o‚ÇÑ = 0.85",
          values: { ot: 0.85 }
        },
        {
          step: "√âtape 9: LSTM - Hidden State Final",
          subtitle: "Calcul h‚ÇÑ",
          description: "√âtat final apr√®s 'Il √©tait une fois'",
          calculation: "h‚ÇÑ = o‚ÇÑ ‚äô tanh(C‚ÇÑ) = 0.85 √ó tanh(1.25) = 0.85 √ó 0.85 = 0.72",
          input: "o‚ÇÑ = 0.85, C‚ÇÑ = 1.25",
          output: "h‚ÇÑ = 0.72 (contexte narratif)",
          values: { ht: 0.72, calculation: "0.85 √ó 0.85 = 0.72" }
        },
        {
          step: "√âtape 10: Pr√©diction - Mot Suivant",
          subtitle: "G√©n√©ration",
          description: "Pr√©dire le prochain mot bas√© sur h‚ÇÑ",
          calculation: "y = softmax(W ¬∑ h‚ÇÑ + b)",
          input: "h‚ÇÑ = 0.72",
          output: "Mot pr√©dit: 'un' (probabilit√©: 0.45)",
          values: { word: "un", probability: 0.45 }
        },
        {
          step: "√âtape 11: LSTM - Traitement 'un'",
          subtitle: "Nouveau mot",
          description: "Traiter le mot g√©n√©r√© 'un'",
          calculation: "Utiliser h‚ÇÑ et embedding('un')",
          input: "h‚ÇÑ = 0.72, x‚ÇÖ = embedding('un')",
          output: "h‚ÇÖ, C‚ÇÖ calcul√©s",
          values: { word: "un" }
        },
        {
          step: "√âtape 12: G√©n√©ration Continue",
          subtitle: "Plusieurs mots",
          description: "Continuer √† g√©n√©rer les mots suivants",
          calculation: "R√©p√©ter le processus",
          input: "Contexte accumul√©",
          output: "Mots suivants: 'prince', 'qui', 'vivait'",
          values: { generated_words: ["prince", "qui", "vivait"] }
        },
        {
          step: "√âtape 13: Texte G√©n√©r√© Final",
          subtitle: "R√©sultat",
          description: "Assembler tous les mots g√©n√©r√©s",
          calculation: "Concat√©nation",
          input: "Texte initial + mots g√©n√©r√©s",
          output: "Texte complet: 'Il √©tait une fois un prince qui vivait dans un ch√¢teau...'",
          values: { full_text: "Il √©tait une fois un prince qui vivait dans un ch√¢teau...", length: 45 }
        }
      ]
    }
  }

  const currentExample = selectedExample ? examples[selectedExample] : null

  const handleExampleSelect = (exampleKey) => {
    setSelectedExample(exampleKey)
    setCurrentStep(0)
    setIsRunning(false)
    if (autoPlayInterval) {
      clearInterval(autoPlayInterval)
      setAutoPlayInterval(null)
    }
  }

  const handleRun = () => {
    if (!currentExample) return
    
    if (isRunning) {
      // Stop
      if (autoPlayInterval) {
        clearInterval(autoPlayInterval)
        setAutoPlayInterval(null)
      }
      setIsRunning(false)
    } else {
      // Start
      setIsRunning(true)
      setCurrentStep(0)
      
      const interval = setInterval(() => {
        setCurrentStep(prev => {
          if (prev < currentExample.steps.length - 1) {
            return prev + 1
          } else {
            clearInterval(interval)
            setIsRunning(false)
            setAutoPlayInterval(null)
            return prev
          }
        })
      }, 3000)
      
      setAutoPlayInterval(interval)
    }
  }

  const handleNext = () => {
    if (currentExample && currentStep < currentExample.steps.length - 1) {
      setCurrentStep(currentStep + 1)
    }
  }

  const handlePrevious = () => {
    if (currentStep > 0) {
      setCurrentStep(currentStep - 1)
    }
  }

  const handleStepClick = (index) => {
    setCurrentStep(index)
  }

  return (
    <div className={`real-examples-container ${presentationMode ? 'presentation-mode' : ''}`}>
      {!presentationMode && (
        <>
          <h2>üåç Exemples R√©els et Ex√©cution D√©taill√©e</h2>
          <p className="examples-intro">
            D√©couvrez comment LSTM fonctionne dans des applications r√©elles avec toutes les √©tapes d√©taill√©es et les calculs r√©els.
          </p>
        </>
      )}

      <div className="examples-grid">
        {Object.entries(examples).map(([key, example]) => (
          <div 
            key={key}
            className={`example-card ${selectedExample === key ? 'selected' : ''}`}
            onClick={() => handleExampleSelect(key)}
          >
            <div className="example-icon">
              {key === 'sentiment' && 'üòä'}
              {key === 'prediction' && 'üìà'}
              {key === 'translation' && 'üåê'}
              {key === 'textGeneration' && '‚úçÔ∏è'}
            </div>
            <h3>{example.title}</h3>
            <p>{example.description}</p>
            <div className="example-badge">
              {example.steps.length} √©tapes d√©taill√©es
            </div>
          </div>
        ))}
      </div>

      {currentExample && (
        <div className="execution-panel">
          <div className="execution-header">
            <div>
              <h3>{currentExample.title}</h3>
              <p className="execution-description">{currentExample.description}</p>
            </div>
            <div className="execution-controls">
              <button onClick={handlePrevious} disabled={currentStep === 0}>
                ‚óÄ Pr√©c√©dent
              </button>
              <button onClick={handleRun} className={isRunning ? 'running' : ''}>
                {isRunning ? '‚è∏ Pause' : '‚ñ∂ Ex√©cuter'}
              </button>
              <button onClick={handleNext} disabled={currentStep === currentExample.steps.length - 1}>
                Suivant ‚ñ∂
              </button>
            </div>
          </div>

          <div className="steps-navigation">
            <div className="steps-list">
              {currentExample.steps.map((step, index) => (
                <div
                  key={index}
                  className={`step-nav-item ${index === currentStep ? 'active' : ''} ${index < currentStep ? 'completed' : ''}`}
                  onClick={() => handleStepClick(index)}
                >
                  <span className="step-nav-number">{index + 1}</span>
                  <span className="step-nav-title">{step.step}</span>
                  {index < currentStep && <span className="step-nav-check">‚úì</span>}
                </div>
              ))}
            </div>
          </div>

          <div className="execution-content">
            <div className="input-section">
              <h4>üì• Entr√©e :</h4>
              <div className="input-box">
                {currentExample.input}
              </div>
            </div>

            <div className="steps-section">
              <div className="step-indicator">
                <div className="step-info">
                  <span className="step-number-large">{currentStep + 1}</span>
                  <span className="step-total">/ {currentExample.steps.length}</span>
                </div>
                <div className="step-progress">
                  <div 
                    className="step-progress-bar" 
                    style={{width: `${((currentStep + 1) / currentExample.steps.length) * 100}%`}}
                  ></div>
                </div>
              </div>

              {currentExample.steps[currentStep] && (
                <div className="execution-step active">
                  <div className="step-header">
                    <div className="step-title-section">
                      <h5>{currentExample.steps[currentStep].step}</h5>
                      {currentExample.steps[currentStep].subtitle && (
                        <p className="step-subtitle">{currentExample.steps[currentStep].subtitle}</p>
                      )}
                    </div>
                    <span className="current-badge">√âtape Active</span>
                  </div>
                  
                  <div className="step-description-box">
                    <p className="step-description">{currentExample.steps[currentStep].description}</p>
                  </div>

                  <div className="step-calculation">
                    <div className="calculation-box">
                      <div className="calculation-label">üìê Calcul :</div>
                      <div className="calculation-formula">{currentExample.steps[currentStep].calculation}</div>
                    </div>
                  </div>

                  {currentExample.steps[currentStep].formula && (
                    <div className="step-formula">
                      <div className="formula-box-detailed">
                        <div className="formula-label">üìê Formule Math√©matique :</div>
                        <pre className="formula-content-detailed">{currentExample.steps[currentStep].formula}</pre>
                      </div>
                    </div>
                  )}

                  <div className="step-io">
                    <div className="io-box input-box-detailed">
                      <div className="io-label">üì• Entr√©e :</div>
                      <div className="io-content">{currentExample.steps[currentStep].input}</div>
                    </div>
                    <div className="io-arrow">‚Üí</div>
                    <div className="io-box output-box-detailed">
                      <div className="io-label">üì§ Sortie :</div>
                      <div className="io-content">{currentExample.steps[currentStep].output}</div>
                    </div>
                  </div>

                  {Object.keys(currentExample.steps[currentStep].values).length > 0 && (
                    <div className="step-values">
                      <div className="values-title">üìä Valeurs Calcul√©es :</div>
                      <div className="values-grid">
                        {Object.entries(currentExample.steps[currentStep].values).map(([key, value]) => (
                          <div key={key} className="value-item">
                            <span className="value-label">{key}:</span>
                            <span className="value-content">
                              {Array.isArray(value) 
                                ? `[${value.join(', ')}]`
                                : typeof value === 'object'
                                ? JSON.stringify(value)
                                : typeof value === 'number' && value < 1 && value > 0
                                ? value.toFixed(3)
                                : typeof value === 'number'
                                ? value.toFixed(2)
                                : value}
                            </span>
                          </div>
                        ))}
                      </div>
                    </div>
                  )}
                </div>
              )}

              {currentStep === currentExample.steps.length - 1 && (
                <div className="execution-complete">
                  <div className="complete-icon">‚úÖ</div>
                  <h4>Ex√©cution Termin√©e !</h4>
                  <p>L'LSTM a trait√© avec succ√®s toutes les √©tapes et produit le r√©sultat final.</p>
                  <div className="complete-summary">
                    <strong>R√©sum√© :</strong> {currentExample.steps[currentExample.steps.length - 1].output}
                  </div>
                </div>
              )}
            </div>
          </div>
        </div>
      )}
    </div>
  )
}

export default RealExamples
